{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5TSqpQoDkGB"
      },
      "source": [
        "### Topic: Latent Semantic Analysis\n",
        "\n",
        "Team members:\n",
        "*   Yash Khandelwal (PID: A00000000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XYUFrXhDLdE"
      },
      "outputs": [],
      "source": [
        "# importspip in\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09wQ7ReYExSD"
      },
      "source": [
        "### Load dataset and print basic stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj6XumCKDyHn"
      },
      "outputs": [],
      "source": [
        "#Load dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "remove=('headers', 'footers', 'quotes')\n",
        "dataset = fetch_20newsgroups(subset='train', remove=remove, shuffle = True)\n",
        "dataset.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlEO0yv2D6N3"
      },
      "outputs": [],
      "source": [
        "# len of training and testing dataset\n",
        "print(\"Length of data:\", len(dataset.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M9_RIQmEFqP"
      },
      "outputs": [],
      "source": [
        "# get the list of 20 labels\n",
        "pp.pprint(dataset.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q0KtOvVLRjC"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgHF-pV1LQ3p"
      },
      "outputs": [],
      "source": [
        "# put it into dataframe\n",
        "news_df = pd.DataFrame({'News': dataset.data,\n",
        "                       'Target': dataset.target})\n",
        "\n",
        "# get dimensions of data\n",
        "news_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdINEPzmLkXs"
      },
      "outputs": [],
      "source": [
        "news_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ09reEqMMu2"
      },
      "outputs": [],
      "source": [
        "news_df['Target_name'] = news_df['Target'].apply(lambda x: dataset.target_names[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtz2nKVPMUK2"
      },
      "outputs": [],
      "source": [
        "news_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuGLOmcJMWMn"
      },
      "outputs": [],
      "source": [
        "# plot distribution of topics in news data\n",
        "fig = plt.figure(figsize=[10,7])\n",
        "ax = sns.countplot(y=news_df['Target_name'], palette='rocket')\n",
        "plt.title('Distribution of Topics')\n",
        "plt.ylabel('Topics')\n",
        "plt.xlabel('Count of topics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGEoKVR4GRCr"
      },
      "source": [
        "### Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2cevR_tN2AY"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ_uHEtENP95"
      },
      "outputs": [],
      "source": [
        "# tokenize\n",
        "# remove non alphabetic characters\n",
        "# remove stopwords and lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjnH59YXNU3c"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "def clean_text(sentence):\n",
        "    # remove non alphabetic sequences\n",
        "    pattern = re.compile(r'[^a-z]+')\n",
        "    sentence = sentence.lower()\n",
        "    sentence = pattern.sub(' ', sentence).strip()\n",
        "\n",
        "    # Tokenize\n",
        "    word_list = word_tokenize(sentence)\n",
        "\n",
        "    # stop words\n",
        "    stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "    # remove stop words\n",
        "    word_list = [word for word in word_list if word not in stopwords_list]\n",
        "\n",
        "    # remove very small words, length < 3 as they don't contribute any useful information\n",
        "    word_list = [word for word in word_list if len(word) > 2]\n",
        "\n",
        "    # lemmatize\n",
        "    lemma = WordNetLemmatizer()\n",
        "    word_list = [lemma.lemmatize(word) for word in word_list]\n",
        "\n",
        "    # list to sentence\n",
        "    sentence = ' '.join(word_list)\n",
        "\n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzDVYkCzNt1d"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "# clean text data\n",
        "news_df['News'] = news_df['News'].progress_apply(lambda x: clean_text(str(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJv0f5yqOCP4"
      },
      "outputs": [],
      "source": [
        "news_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnmYXc3cOFTN"
      },
      "outputs": [],
      "source": [
        "# plot word count for news text\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                      max_words=200).generate(str(news_df['News']))\n",
        "fig = plt.figure(figsize=[16,16])\n",
        "plt.title('WordCloud of News')\n",
        "plt.axis('off')\n",
        "plt.imshow(wordcloud)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1867rBm8OToB"
      },
      "outputs": [],
      "source": [
        "# vectorize text data\n",
        "tfid_vec = TfidfVectorizer(tokenizer=lambda x: str(x).split(), max_df=0.95, min_df=2)\n",
        "X = tfid_vec.fit_transform(news_df['News'])\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guAy3a3AOcDB"
      },
      "source": [
        "### Latent Semantic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgaWRL5bObbP"
      },
      "outputs": [],
      "source": [
        "# create svd instance\n",
        "svd_model = TruncatedSVD(n_components=20,\n",
        "                         algorithm='randomized')\n",
        "\n",
        "# fit model to data\n",
        "svd_model.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IplQ_FneOXmL"
      },
      "outputs": [],
      "source": [
        "# topic word mapping martrix\n",
        "svd_model.components_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm8krrN3Oq2K"
      },
      "outputs": [],
      "source": [
        "# document topic mapping matrix\n",
        "doc_topic = svd_model.fit_transform(X)\n",
        "doc_topic.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISpUGYIuO8qJ"
      },
      "outputs": [],
      "source": [
        "terms = tfid_vec.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvXqk4aROtyi"
      },
      "outputs": [],
      "source": [
        "# function to map words to topics\n",
        "def map_word2topic(components, terms):\n",
        "    # create output series\n",
        "    word2topics = pd.Series()\n",
        "\n",
        "    for idx, component in enumerate(components):\n",
        "        # map terms (words) with topic\n",
        "        # which is probability of word given a topic P(w|t)\n",
        "        term_topic = pd.Series(component, index=terms)\n",
        "        # sort values based on probability\n",
        "        term_topic.sort_values(ascending=False, inplace=True)\n",
        "        # put result in series output\n",
        "        word2topics['topic '+str(idx)] = list(term_topic.iloc[:5].index)\n",
        "\n",
        "    return word2topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-2NC1izOzOl"
      },
      "outputs": [],
      "source": [
        "word2topics = map_word2topic(svd_model.components_, terms)\n",
        "\n",
        "# print topic results\n",
        "print('Topics\\t\\tWords')\n",
        "for idx, item in zip(word2topics.index, word2topics):\n",
        "    print(idx,'\\t',item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnma3EwBO4AB"
      },
      "outputs": [],
      "source": [
        "def get_top3_topics(x):\n",
        "    top3 = list(x.sort_values(ascending=False).head(3).index) + list(x.sort_values(ascending=False).head(3).values)\n",
        "    return top3\n",
        "\n",
        "# map top3 topic words to news document\n",
        "def map_topicword2doc(model, X):\n",
        "    # output data frame column list\n",
        "    cols = ['topic_'+str(i+1)+'_name' for i in range(3)] + ['topic_'+str(i+1)+'_prob' for i in range(3)]\n",
        "    # doc to topic mapping\n",
        "    doc_topic = model.fit_transform(X)\n",
        "    # list of topics\n",
        "    topics = ['topic'+str(i) for i in range(20)]\n",
        "    # doc topic data frame\n",
        "    doc_topic_df = pd.DataFrame(doc_topic, columns=topics)\n",
        "    # map top 3 topics to doc\n",
        "    outdf = doc_topic_df.progress_apply(lambda x: get_top3_topics(x), axis=1)\n",
        "    # outdf is a series of list\n",
        "    # convert it to a data frame\n",
        "    outdf = pd.DataFrame(dict(zip(outdf.index, outdf.values))).T\n",
        "    outdf.columns = cols\n",
        "\n",
        "    return outdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gxSFwS4PCmI"
      },
      "outputs": [],
      "source": [
        "top_topics = map_topicword2doc(svd_model, X)\n",
        "news_topics = pd.concat([news_df, top_topics], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZgB0qcIPDze"
      },
      "outputs": [],
      "source": [
        "# convert probability from string to float\n",
        "news_topics = news_topics.infer_objects()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqobQ2ovPGtZ"
      },
      "outputs": [],
      "source": [
        "news_topics.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2i7fN66PIFl"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[10,7])\n",
        "ax = sns.countplot(y=news_topics['topic_1_name'], palette='rocket')\n",
        "plt.title('Distribution of Topics 1')\n",
        "plt.ylabel('Topics')\n",
        "plt.xlabel('Count of topic 1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0GYiY2KcnD0"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[10,7])\n",
        "ax = sns.countplot(y=news_topics['topic_2_name'], palette='rocket')\n",
        "plt.title('Distribution of Topics 2')\n",
        "plt.ylabel('Topics')\n",
        "plt.xlabel('Count of topic 2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qae8YIaIco_T"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[10,7])\n",
        "ax = sns.countplot(y=news_topics['topic_3_name'], palette='rocket')\n",
        "plt.title('Distribution of Topics 3')\n",
        "plt.ylabel('Topics')\n",
        "plt.xlabel('Count of topic 3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5B4U3MBJM9-"
      },
      "outputs": [],
      "source": [
        "X_topics = svd_model.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "HHMoQy7K8YcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BumcF2qprq4w"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1],\n",
        "c = dataset.target,\n",
        "s = 10, # size\n",
        "edgecolor='none' )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBsPHbBh6VMv"
      },
      "source": [
        "### Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p39Svhfsscmq"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda_model = LatentDirichletAllocation(n_components=20,\n",
        "                                     max_iter=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDJEhyed6YWW"
      },
      "outputs": [],
      "source": [
        "lda_model.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLlDL5MN7twD"
      },
      "outputs": [],
      "source": [
        "doc_topic_lda = lda_model.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJUpZS656aAU"
      },
      "outputs": [],
      "source": [
        "word2topics_lda = map_word2topic(lda_model.components_, terms)\n",
        "\n",
        "# print topic results\n",
        "print('Topics\\t\\tWords')\n",
        "for idx, item in zip(word2topics_lda.index, word2topics_lda):\n",
        "    print(idx,'\\t',item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhhE49VS7Apv"
      },
      "outputs": [],
      "source": [
        "def get_top3_topics(x):\n",
        "    top3 = list(x.sort_values(ascending=False).head(3).index) + list(x.sort_values(ascending=False).head(3).values)\n",
        "    return top3\n",
        "\n",
        "# map top3 topic words to news document\n",
        "def map_topicword2doc(model, X):\n",
        "    # output data frame column list\n",
        "    cols = ['topic_'+str(i+1)+'_name' for i in range(3)] + ['topic_'+str(i+1)+'_prob' for i in range(3)]\n",
        "    # doc to topic mapping\n",
        "    doc_topic = model.fit_transform(X)\n",
        "    # list of topics\n",
        "    topics = ['topic'+str(i) for i in range(20)]\n",
        "    # doc topic data frame\n",
        "    doc_topic_df = pd.DataFrame(doc_topic, columns=topics)\n",
        "    # map top 3 topics to doc\n",
        "    outdf = doc_topic_df.progress_apply(lambda x: get_top3_topics(x), axis=1)\n",
        "    # outdf is a series of list\n",
        "    # convert it to a data frame\n",
        "    outdf = pd.DataFrame(dict(zip(outdf.index, outdf.values))).T\n",
        "    outdf.columns = cols\n",
        "\n",
        "    return outdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAYirCtW8lLS"
      },
      "outputs": [],
      "source": [
        "top_topics = map_topicword2doc(lda_model, X)\n",
        "news_topics = pd.concat([news_df, top_topics], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4S9NUvO8n29"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[10,7])\n",
        "ax = sns.countplot(y=news_topics['topic_1_name'], palette='rocket')\n",
        "plt.title('Distribution of Topics 1')\n",
        "plt.ylabel('Topics')\n",
        "plt.xlabel('Count of topics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjy1Nf2Z7wcg"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[10,7])\n",
        "ax = sns.countplot(y=news_topics['topic_2_name'], palette='rocket')\n",
        "plt.title('Distribution of Topics 2')\n",
        "plt.ylabel('Topics')\n",
        "plt.xlabel('Count of topics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okTxTY6l7wcg"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[10,7])\n",
        "ax = sns.countplot(y=news_topics['topic_3_name'], palette='rocket')\n",
        "plt.title('Distribution of Topics 3')\n",
        "plt.ylabel('Topics')\n",
        "plt.xlabel('Count of topics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfwpZaMu8p6W"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1],\n",
        "c = dataset.target,\n",
        "s = 10, # size\n",
        "edgecolor='none' )\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "guAy3a3AOcDB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "firstEnv",
      "language": "python",
      "name": "firstenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}